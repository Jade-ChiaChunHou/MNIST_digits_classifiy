{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST Analysis",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z6UHmLYVhWAN"
      },
      "source": [
        "# MNIST Digit Classification with KNN and Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iJ9ayCvyhWAP",
        "colab": {}
      },
      "source": [
        "# This tells matplotlib not to try opening a new window for each plot.\n",
        "%matplotlib inline\n",
        "\n",
        "# Import a bunch of libraries.\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MultipleLocator\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Set the randomizer seed so results are the same each time.\n",
        "np.random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCObK36QfXFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn\n",
        "sklearn.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sO1t0ypThWAR"
      },
      "source": [
        "Load the data. Notice that the data gets partitioned into training, development, and test sets. Also, a small subset of the training data called mini_train_data and mini_train_labels gets defined, which you should use in all the experiments below, unless otherwise noted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3yK9DacchWAS",
        "colab": {}
      },
      "source": [
        "# Load the digit data from https://www.openml.org/d/554 or from default local location '~/scikit_learn_data/...'\n",
        "X, Y = fetch_openml(name='mnist_784', return_X_y=True, cache=False)\n",
        "\n",
        "\n",
        "# Rescale grayscale values to [0,1].\n",
        "X = X / 255.0\n",
        "\n",
        "# Shuffle the input: create a random permutation of the integers between 0 and the number of data points and apply this\n",
        "# permutation to X and Y.\n",
        "# NOTE: Each time you run this cell, you'll re-shuffle the data, resulting in a different ordering.\n",
        "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
        "X, Y = X[shuffle], Y[shuffle]\n",
        "\n",
        "print('data shape: ', X.shape)\n",
        "print('label shape:', Y.shape)\n",
        "\n",
        "# Set some variables to hold test, dev, and training data.\n",
        "test_data, test_labels = X[61000:], Y[61000:]\n",
        "dev_data, dev_labels = X[60000:61000], Y[60000:61000]\n",
        "train_data, train_labels = X[:60000], Y[:60000]\n",
        "mini_train_data, mini_train_labels = X[:1000], Y[:1000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "atc2JpWKhWAV"
      },
      "source": [
        "### Part 1:\n",
        "\n",
        "Show a 10x10 grid that visualizes 10 examples of each digit.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JjI30P4I_vb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def P1(num_examples):\n",
        "\n",
        "  fig = plt.figure(figsize=(num_examples*3,num_examples*3))\n",
        "\n",
        "  for x in range(num_examples):\n",
        "    for y in range(num_examples):\n",
        "      ax = fig.add_subplot(num_examples, num_examples, num_examples*y+x+1)\n",
        "      plt.rc('image', cmap='gray')\n",
        "      ax.matshow(X[np.where(Y == str(y))[0][x]].reshape((28,28)))\n",
        "      plt.title('Digit Label: {}'.format(str(y)))\n",
        "      plt.xticks(np.array([]))\n",
        "      plt.yticks(np.array([]))\n",
        "\n",
        "P1(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EMQAHr7QhWAX"
      },
      "source": [
        "### Part 2:\n",
        "\n",
        "Produce k-Nearest Neighbors models with k $\\in$ [1,3,5,7,9].  Evaluate and show the accuracy of each model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-it5pn8-hWAY",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def P2(k_values):\n",
        "\n",
        "  for k in k_values:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(mini_train_data, mini_train_labels)\n",
        "    y_pred = knn.predict(dev_data)\n",
        "\n",
        "    acc = accuracy_score(dev_labels, y_pred)\n",
        "    print('k = %d, accuracy: %3.3f' %(k, acc))\n",
        "\n",
        "    if k == 1:\n",
        "      print(classification_report(dev_labels, y_pred))\n",
        "\n",
        "k_values = [1, 3, 5, 7, 9]\n",
        "P2(k_values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7b6YEAzzhWAa"
      },
      "source": [
        "### Part 3:\n",
        "\n",
        "Produce 1-Nearest Neighbor models using training data of various sizes.  Evaluate and show the performance of each model.  Additionally, show the time needed to measure the performance of each model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gEpNzDEjhWAa",
        "colab": {}
      },
      "source": [
        "import timeit\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def P3(train_sizes, accuracies):\n",
        "\n",
        "  for sizes in train_sizes:\n",
        "    \n",
        "    start = timeit.default_timer()\n",
        "    knn = KNeighborsClassifier(n_neighbors=1)\n",
        "    knn.fit(train_data[0:sizes], train_labels[0:sizes])\n",
        "    y_pred = knn.predict(dev_data)\n",
        "    stop = timeit.default_timer()\n",
        "    acc = accuracy_score(dev_labels, y_pred)\n",
        "    accuracies.append(acc)\n",
        "    print('train sizes = %d, accuracy: %3.3f, time: %s' %(sizes, acc, stop - start))\n",
        "\n",
        "train_sizes = [100, 200, 400, 800, 1600, 3200, 6400, 12800, 25600]\n",
        "accuracies = []\n",
        "P3(train_sizes, accuracies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B56lVsKNhWAc"
      },
      "source": [
        "### Part 4:\n",
        "\n",
        "Produce a linear regression model that predicts accuracy of a 1-Nearest Neighbor model given training set size. Show $R^2$ of the linear regression model.  Show the accuracies predicted for training set sizes 60000, 120000, and 1000000.  Show a lineplot of actual accuracies and predicted accuracies vs. training set size over the range of training set sizes in the training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4xE_qIJghWAc",
        "colab": {}
      },
      "source": [
        "def P4():\n",
        "\n",
        "  train_sizes = [100, 200, 400, 800, 1600, 3200, 6400, 12800, 25600, 60000, 120000, 1000000]\n",
        "  accuracies = []\n",
        "\n",
        "  for sizes in train_sizes:\n",
        "    \n",
        "    knn = KNeighborsClassifier(n_neighbors=1)\n",
        "    knn.fit(train_data[0:sizes], train_labels[0:sizes])\n",
        "    y_pred = knn.predict(dev_data)\n",
        "    acc = accuracy_score(dev_labels, y_pred)\n",
        "    accuracies.append(acc)\n",
        "    print('train sizes = %d, accuracy: %3.3f' %(sizes, acc))\n",
        "\n",
        "  sizes = [[100], [200], [400], [800], [1600], [3200], [6400], [12800], [25600], [60000],[120000],[1000000]]\n",
        "  sizes = np.array(sizes)  \n",
        "\n",
        "  # Untransform\n",
        "  lr = LinearRegression()\n",
        "  lr.fit(sizes, accuracies)\n",
        "  print(\"Untransform model, R-squared: \",lr.score(sizes, accuracies))\n",
        "\n",
        "  # Initialize a new plot.\n",
        "  plt.figure(figsize=(8, 4))\n",
        "\n",
        "  # Show samples from the fitted function.\n",
        "  plt.plot(sizes, lr.predict(sizes), label=\"Untransform model\")\n",
        "\n",
        "  # Show the original noisy samples.\n",
        "  plt.scatter(sizes, accuracies, label=\"Accuracies\")\n",
        "\n",
        "  # Add a few more labels to the plot.\n",
        "  plt.xlabel(\"Training set sizes\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.legend(loc=\"best\")\n",
        "\n",
        "  # Render the plots.\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  # Transform\n",
        "  lr = LinearRegression()\n",
        "  lr.fit(np.log(sizes), accuracies)\n",
        "  print(\"Log transform model, R-squared: \",lr.score(np.log(sizes), accuracies))\n",
        "\n",
        "  # Initialize a new plot.\n",
        "  plt.figure(figsize=(8, 4))\n",
        "\n",
        "  # Show samples from the fitted function.\n",
        "  plt.plot(np.log(sizes), lr.predict(np.log(sizes)), label=\"Log transform model\")\n",
        "\n",
        "  # Show the original noisy samples.\n",
        "  plt.scatter(np.log(sizes), accuracies, label=\"Accuracies\")\n",
        "\n",
        "  # Add a few more labels to the plot.\n",
        "  plt.xlabel(\"Training set sizes (2^x)\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.legend(loc=\"best\")\n",
        "\n",
        "  # Render the plots.\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "P4()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "geAQJjGRhWAe"
      },
      "source": [
        "### Part 5:\n",
        "\n",
        "Produce a 1-Nearest Neighbor model and show the confusion matrix.  Show the images of these most often confused digits.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bq36xaQohWAf",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import classification_report \n",
        "\n",
        "def P5():\n",
        "\n",
        "  k = 1\n",
        "  knn = KNeighborsClassifier(n_neighbors=k)\n",
        "  knn.fit(mini_train_data, mini_train_labels)\n",
        "  y_pred = knn.predict(dev_data)\n",
        "\n",
        "  print(\"The confusion matrix of 1-Nearest Neighbor model.\")\n",
        "  print(confusion_matrix(dev_labels, y_pred))\n",
        "  print()\n",
        "  result = confusion_matrix(dev_labels, y_pred)\n",
        "\n",
        "  error_num = []\n",
        "  for i in range(len(result)):\n",
        "    error_num.append(result[i].sum() - result[i][i])\n",
        "\n",
        "  digits = [i for i, x in enumerate(error_num) if x == max(error_num)]\n",
        "\n",
        "  for i in digits:\n",
        "    print(i, \"is the digit that 1-Nearest Neighbor confuse the most.\")\n",
        "\n",
        "  \n",
        "  # error digits in 2 & 8\n",
        "  index2 = list(np.where((y_pred!= dev_labels) & (dev_labels == \"2\"))[0])\n",
        "  index8 = list(np.where((y_pred!= dev_labels) & (dev_labels == \"8\"))[0])\n",
        "  index = index2 + index8\n",
        "\n",
        "  for i in range(len(index)):\n",
        "    plt.rc('image', cmap='gray')\n",
        "    img = dev_data[index[i]].reshape((28,28))\n",
        "    plt.title('Label: {}, Predict: {}'.format(str(dev_labels[index[i]]), str(y_pred[index[i]])))\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "P5()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tgqMKb-hhWAh"
      },
      "source": [
        "### Part 6:\n",
        "Smooth an image by blurring."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lSKHmHGshWAi",
        "colab": {}
      },
      "source": [
        "from scipy.ndimage.filters import gaussian_filter\n",
        "\n",
        "def P6():\n",
        "    \n",
        "### STUDENT START ###\n",
        "  k = 1\n",
        "  blur_mini_train_data = gaussian_filter(mini_train_data, sigma = 1, order = 0, mode = \"constant\", truncate = 1, cval = 2.0)\n",
        "  blur_dev_data = gaussian_filter(dev_data, sigma = 1)\n",
        "\n",
        "  # no filter\n",
        "  knn = KNeighborsClassifier(n_neighbors=k)\n",
        "  knn.fit(mini_train_data, mini_train_labels)\n",
        "  y_pred = knn.predict(dev_data)\n",
        "  print(\"No filter, accuracy: \", accuracy_score(dev_labels, y_pred))\n",
        "\n",
        "  # Filter the training data but not the dev data\n",
        "  knn = KNeighborsClassifier(n_neighbors=k)\n",
        "  knn.fit(blur_mini_train_data, mini_train_labels)\n",
        "  y_pred = knn.predict(dev_data)\n",
        "  print(\"Filter the training data but not the dev data, accuracy: \", accuracy_score(dev_labels, y_pred))\n",
        "\n",
        "  # Filter the dev data but not the training data\n",
        "  knn = KNeighborsClassifier(n_neighbors=k)\n",
        "  knn.fit(mini_train_data, mini_train_labels)\n",
        "  y_pred = knn.predict(blur_dev_data)\n",
        "  print(\"Filter the dev data but not the training data, accuracy: \", accuracy_score(dev_labels, y_pred))\n",
        "\n",
        "  # Filter both training data and dev data\n",
        "  knn = KNeighborsClassifier(n_neighbors=k)\n",
        "  knn.fit(blur_mini_train_data, mini_train_labels)\n",
        "  y_pred = knn.predict(blur_dev_data)\n",
        "  print(\"Filter both training data and dev data, accuracy: \", accuracy_score(dev_labels, y_pred))\n",
        "\n",
        "### STUDENT END ###\n",
        "\n",
        "P6()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LtgepWfAhWAk"
      },
      "source": [
        "### Part 7:\n",
        "\n",
        "Produce two Naive Bayes models and evaluate their performances.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eGpH-4IQhWAk",
        "colab": {}
      },
      "source": [
        "def P7():\n",
        "\n",
        "  # Bernoulli model\n",
        "  #prepocess mini_train_data\n",
        "  b_mini_train = mini_train_data[:]\n",
        "  b_mini_train[b_mini_train >= 0.1] = 1\n",
        "  b_mini_train[b_mini_train < 0.1] = 0\n",
        "\n",
        "  #prepocess dev_data\n",
        "  b_dev = dev_data[:]\n",
        "  b_dev[b_dev >= 0.1] = 1\n",
        "  b_dev[b_dev < 0.1] = 0\n",
        "\n",
        "  # fit model\n",
        "  clf = BernoulliNB()\n",
        "  clf.fit(b_mini_train, mini_train_labels)\n",
        "  print ('Bernoulli model accuracy: %3.3f' %clf.score(b_dev, dev_labels))\n",
        "\n",
        "  # Multinomial model\n",
        "  #prepocess mini_train_data\n",
        "  m_mini_train = mini_train_data[:]\n",
        "  m_mini_train[m_mini_train >= 0.9] = 2\n",
        "  m_mini_train[(m_mini_train >= 0.1) & (m_mini_train < 0.9)] = 1\n",
        "  m_mini_train[m_mini_train < 0.1] = 0\n",
        "\n",
        "  #prepocess dev_data\n",
        "  m_dev = dev_data[:]\n",
        "  m_dev[m_dev >= 0.9] = 2\n",
        "  m_dev[(m_dev >= 0.1) & (m_dev < 0.9)] = 1\n",
        "  m_dev[m_dev < 0.1] = 0\n",
        "\n",
        "  # fit model\n",
        "\n",
        "  clf = MultinomialNB()\n",
        "  clf.fit(m_mini_train, mini_train_labels)\n",
        "  print ('Multinomial model accuracy: %3.3f' %clf.score(m_dev, dev_labels))\n",
        "\n",
        "P7()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PqjbRLg7hWAm"
      },
      "source": [
        "### Part 8:\n",
        "\n",
        "Search across several values of the LaPlace smoothing parameter (alpha) to find its effect on a Bernoulli Naive Bayes model's performance.  Show the accuracy at each alpha value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0AvZ-Wp3hWAn",
        "colab": {}
      },
      "source": [
        "def P8(alphas):\n",
        "\n",
        "  b_nb = BernoulliNB(binarize=0.0)\n",
        "  clf = GridSearchCV(b_nb, alphas, cv= 5, scoring='accuracy', iid=False)\n",
        "  clf.fit(mini_train_data, mini_train_labels)\n",
        "\n",
        "  \n",
        "  print ('accuracy: %3.3f' %clf.score(dev_data, dev_labels))\n",
        "  clf.best_params_\n",
        "\n",
        "  return clf\n",
        "\n",
        "alphas = {'alpha': [1.0e-10, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]}\n",
        "nb = P8(alphas)\n",
        "print()\n",
        "print(\"Best alpha = \", nb.best_params_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B07GDiDdhWAq"
      },
      "source": [
        "### Part 9:\n",
        "\n",
        "Produce a model using Guassian Naive Bayes, which is intended for real-valued features, and evaluate performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gBLbTMWChWAq",
        "colab": {}
      },
      "source": [
        "def P9():\n",
        "\n",
        "  gnb = GaussianNB()\n",
        "  gnb_fit = gnb.fit(mini_train_data, mini_train_labels)\n",
        "  print('Accuracy: %f' %gnb.score(dev_data, dev_labels))\n",
        "\n",
        "P9()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dgZMuc1VhWAt"
      },
      "source": [
        "### Part 10:\n",
        "\n",
        "Naive Bayes produces a generative model, use it to generate digit images.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ktii-Mp-hWAu",
        "colab": {}
      },
      "source": [
        "def P10(num_examples):\n",
        "\n",
        "  b_nb = BernoulliNB(binarize=0.5)\n",
        "  b_nb_fit = b_nb.fit(mini_train_data, mini_train_labels)\n",
        "\n",
        "  def trans_image(pix_prob):\n",
        "      return(np.where(np.random.rand(1, 784) < np.exp(pix_prob),1, 0))\n",
        "\n",
        "  # Generate plots\n",
        "  for index, number in enumerate(b_nb_fit.feature_log_prob_, 0):\n",
        "      for rep in range(0, num_examples):\n",
        "          plot_number = (rep + (index * num_examples)) + 1\n",
        "          plt.subplot(10,num_examples,plot_number)\n",
        "          frame1 = plt.gca()\n",
        "          frame1.axes.get_xaxis().set_visible(False)\n",
        "          frame1.axes.get_yaxis().set_visible(False)\n",
        "          plt.imshow(trans_image(number).reshape(28,28), cmap = 'Greys')\n",
        "\n",
        "P10(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ksHMg73uhWAx"
      },
      "source": [
        "### Part 11:\n",
        "\n",
        "Recall that a strongly calibrated classifier is rougly 90% accurate when the posterior probability of the predicted class is 0.9. A weakly calibrated classifier is more accurate when the posterior probability of the predicted class is 90% than when it is 80%. A poorly calibrated classifier has no positive correlation between posterior probability and accuracy.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a1N-St12hWAy",
        "colab": {}
      },
      "source": [
        "def P11(buckets, correct, total):\n",
        "    \n",
        "  index = 0\n",
        "\n",
        "  for i in buckets:\n",
        "    b_nb = BernoulliNB(alpha = 0.01, binarize=0.0)\n",
        "    clf = b_nb.fit(mini_train_data, mini_train_labels)\n",
        "    preds = b_nb.predict(dev_data[0:int(dev_data.shape[0]*i)]) \n",
        "    correct[index] = np.sum(preds == dev_labels[0:int(dev_data.shape[0]*i)])\n",
        "    total[index] = int(dev_data.shape[0]*i)\n",
        "    index += 1\n",
        "\n",
        "buckets = [0.5, 0.9, 0.999, 0.99999, 0.9999999, 0.999999999, 0.99999999999, 0.9999999999999, 1.0]\n",
        "correct = [0 for i in buckets]\n",
        "total = [0 for i in buckets]\n",
        "\n",
        "P11(buckets, correct, total)\n",
        "\n",
        "for i in range(len(buckets)):\n",
        "  accuracy = 0.0\n",
        "  if (total[i] > 0): accuracy = correct[i] / total[i]\n",
        "  print('p(pred) is %.13f to %.13f    total = %3d    accuracy = %.3f' % (0 if i==0 else buckets[i-1], buckets[i], total[i], accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}